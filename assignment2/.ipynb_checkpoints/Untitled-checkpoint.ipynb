{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-042215ca4f89>, line 206)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-042215ca4f89>\"\u001b[1;36m, line \u001b[1;32m206\u001b[0m\n\u001b[1;33m    model = LinearSVC(C=c, kernel = )\u001b[0m\n\u001b[1;37m                                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# import some library\n",
    "# -*- coding: utf-8 -*\n",
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification as mk\n",
    "#model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from naiveBayes import Berboulli_Naive_Bayes\n",
    "from sklearn.ensemble import BaggingClassifier as BC\n",
    "\n",
    "#help_clean\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "#help_feature_process\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import bigrams\n",
    "#help_analysis\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "#analysis\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from statistics import mean\n",
    "\n",
    "\n",
    "#read from the file, possible write for testing\n",
    "class Reader:\n",
    "\n",
    "    def read(self,path):\n",
    "        data = pd.read_csv(path, encoding=\"utf-8\")\n",
    "        return data\n",
    "\n",
    "    def shuffle(self,df):\n",
    "        df.shuffle()\n",
    "\n",
    "    def write(self,name):\n",
    "        f = open(name, \"w\")\n",
    "        for line in self.data:\n",
    "            f.write(line)\n",
    "        f.close()\n",
    "\n",
    "    def extractColToString(self,df,col_name):\n",
    "        data_p = [ (line) for line in self.file]\n",
    "        return data_p\n",
    "\n",
    "# 2 Cleaning of the data\n",
    "class Cleaner:\n",
    "    def __init__(self, sample_list,use_lemmer,use_stemmer, use_stopwords):\n",
    "        self.sents_list = sample_list\n",
    "        self.words_list = [self.splitter(w) for w in sample_list]\n",
    "        self.s =use_stemmer\n",
    "        self.l =use_lemmer\n",
    "        self.st = use_stopwords\n",
    "\n",
    "    def splitter(self,sample_list):\n",
    "        pos_words = sample_list.split()\n",
    "        return pos_words\n",
    "\n",
    "    def remove_punc(self):\n",
    "        removed_punc = []\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        for s in self.words_list:\n",
    "            removed_punc.append( [w.translate(table) for w in s] )\n",
    "        self.words_list = removed_punc\n",
    "\n",
    "    def lowercase(self):\n",
    "        lowered = []\n",
    "        for s in self.words_list:\n",
    "            lowered.append( [w.lower() for w in s])\n",
    "        self.words_list = lowered\n",
    "\n",
    "    def remove_noncharacter(self):\n",
    "        remove_nonchar = []\n",
    "        for s in self.words_list:\n",
    "            remove_nonchar.append([w for w in s if w.isalnum()])\n",
    "        self.words_list = remove_nonchar\n",
    "\n",
    "    def remove_stopWord(self):\n",
    "        removed_stop = []\n",
    "        stop_words = stopwords.words('english')\n",
    "        for s in self.words_list:\n",
    "            removed_stop.append([w for w in s if not w in stop_words])\n",
    "        self.words_list = removed_stop\n",
    "\n",
    "    def lemmatizer(self):\n",
    "        lemmatized = []\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        for s in self.words_list:\n",
    "            lemmatized.append([lemmatizer.lemmatize(w) for w in s])\n",
    "        self.words_list = lemmatized\n",
    "\n",
    "    def stemmer(self):\n",
    "        stemmed = []\n",
    "        porter = PorterStemmer()\n",
    "        for s in self.words_list:\n",
    "            stemmed.append( [porter.stem(word) for word in s])\n",
    "        self.words_list = stemmed\n",
    "\n",
    "    def clean_low_puc_nc_le_stop(self):\n",
    "        cleaned = []\n",
    "        #porter = PorterStemmer()\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        stop_words = stopwords.words('english')\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        for s in self.words_list:\n",
    "            cleaned.append([lemmatizer.lemmatize(word.translate(table).lower()) for word in s if word not in stop_words])\n",
    "        self.words_list = cleaned\n",
    "\n",
    "    def cleaned(self):\n",
    "        self.lowercase()\n",
    "        self.remove_punc()\n",
    "        self.remove_noncharacter()\n",
    "        if self.l:\n",
    "            self.lemmatizer()\n",
    "        if self.s:\n",
    "            self.stemmer()\n",
    "        if self.st:\n",
    "            self.remove_stopWord()\n",
    "        result = self.joined()\n",
    "        return result\n",
    "\n",
    "    def joined(self):\n",
    "        sents = []\n",
    "        for s in self.words_list:\n",
    "            sents.append(' '.join(s))\n",
    "        return sents\n",
    "# 3 feature processing\n",
    "\n",
    "class Feature_Processer:\n",
    "    def split(self,features_set,target_set, ratio):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(features_set, target_set, train_size=ratio,\n",
    "                                                            test_size=1-ratio)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    #n_grams, min_df\n",
    "    #adjustable (1,2) is not good as (1,1)\n",
    "    def count_vector_features_produce(self, X_train, X_test, thresold):\n",
    "        cv = CountVectorizer(binary=True,min_df=thresold)\n",
    "        cv.fit(X_train)\n",
    "        X = cv.transform(X_train)\n",
    "        X_test = cv.transform(X_test)\n",
    "        return X, X_test\n",
    "\n",
    "    def tf_idf(self,X_train,X_test,n_grams,thresold):\n",
    "        tf_idf_vectorizer = TfidfVectorizer(ngram_range=n_grams,min_df =thresold)\n",
    "        vectors_train_idf = tf_idf_vectorizer.fit_transform(X_train)\n",
    "        vectors_test_idf = tf_idf_vectorizer.transform(X_test)\n",
    "        return vectors_train_idf,vectors_test_idf\n",
    "    #not finished yet\n",
    "    #check later\n",
    "    \"\"\"\n",
    "    def bigram_extractor(self):\n",
    "        bigramFeatureVector = []\n",
    "        for item in bigrams(tweetString.split()):\n",
    "            bigramFeatureVector.append(' '.join(item))\n",
    "        return bigramFeatureVector\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "class classifier:\n",
    "    def __init__(self, x_train, x_test, y_train,y_test):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "\n",
    "    def logistic(self, c):\n",
    "        model = LogisticRegression(C=c, dual=False, solver='lbfgs',multi_class= 'multinomial',max_iter= 2000)\n",
    "        model.fit(self.x_train, self.y_train)\n",
    "        preds = model.predict(self.x_test)\n",
    "        scores1 = cross_val_score(model, self.x_train, self.y_train, cv=5, scoring='accuracy')\n",
    "        print(\"Score of Logistic in Cross Validation\", scores1.mean() * 100)\n",
    "        print(\"Losistic Regression : accurancy_matrix is\", metrics.accuracy_score(self.y_test, preds))\n",
    "        #cm = confusion_matrix(self.y_test, preds)\n",
    "        #print(\"Confusion Matrix\\n\", cm)\n",
    "        #print(\"Report\", classification_report(self.y_test, preds))\n",
    "        \n",
    "    def SelfNaiveByes(self):\n",
    "        model = Berboulli_Naive_Bayes()\n",
    "        model.train(self.x_train,self.y_train)\n",
    "        print(model.score(self.x_test,self.y_test))\n",
    "        \n",
    "    def Ber_NaiveBayes(self, alpha):\n",
    "        model = BernoulliNB(alpha=alpha).fit(self.x_train, self.y_train)\n",
    "        preds = model.predict(self.x_test)\n",
    "        scores2 = cross_val_score(model, self.x_train, self.y_train, cv=5, scoring='accuracy')\n",
    "        print(\"Score of Naive Bayes\", scores2.mean() * 100)\n",
    "        print(\"Bernoulli Naive Bayes : accurancy_matrix is\", metrics.accuracy_score(self.y_test, preds))\n",
    "        cm = confusion_matrix(self.y_test, preds)\n",
    "        print(\"Confusion Matrix\\n\", cm)\n",
    "        print(\"Report\", classification_report(self.y_test, preds))\n",
    "\n",
    "    def svm(self, c, kernel):\n",
    "        model = LinearSVC(C=c, kernel = kernel)\n",
    "        model.fit(self.x_train, self.y_train)\n",
    "        preds = model.predict(self.x_test)\n",
    "\n",
    "        scores3 = cross_val_score(model, self.x_train, self.y_train, cv=5, scoring='accuracy')\n",
    "        print(\"Score of SVM in Cross Validation\", scores3.mean() * 100)\n",
    "        print(\"SVM Regression : accurancy_is\", metrics.accuracy_score(self.y_test, preds))\n",
    "        cm = confusion_matrix(self.y_test, preds)\n",
    "        print(\"Confusion Matrix\\n\", cm)\n",
    "        print(\"Report\", classification_report(self.y_test, preds))\n",
    "\n",
    "    def decision_tree(self):\n",
    "        #criterion=’gini’, splitter=’best’, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort=False\n",
    "        model = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None,min_samples_split=0.1)\n",
    "        model.fit(self.x_train, self.y_train)\n",
    "        scores3 = cross_val_score(model, self.x_train, self.y_train, cv=5, scoring='accuracy')\n",
    "        print(\"Score of decision tree in Cross Validation\", scores3.mean() * 100)\n",
    "        print(\"decision tree  : accurancy_is\", metrics.accuracy_score(self.y_test, model.predict(self.x_test)))\n",
    "\n",
    "    def dummy(self):\n",
    "        clf = DummyClassifier(strategy='stratified', random_state=0)\n",
    "        clf.fit(self.x_train, self.y_train)\n",
    "        score = clf.score(self.x_test, self.y_test)\n",
    "        print(\"Random Baseline's accurancy\", score)\n",
    "\n",
    "    def multNB(self):\n",
    "        model = MultinomialNB()\n",
    "        model.fit(self.x_train, self.y_train)\n",
    "        #scores3 = cross_val_score(model, self.x_train, self.y_train, cv=5, scoring='accuracy')\n",
    "        #print(\"Score of MultinomialNB in Cross Validation\", scores3.mean() * 100)\n",
    "        print(\" MultinomialNB Regression : accurancy_is\", metrics.accuracy_score(self.y_test, model.predict(self.x_test)))\n",
    "\n",
    "    def NB(self):\n",
    "        model =Berboulli_Naive_Bayes()\n",
    "        model.train(self.x_train,self.y_train)\n",
    "        model.fit(self.x_train)\n",
    "        \n",
    "    def BC(self):\n",
    "        model = BC(LinearSVC(C=0.2),max_features= 0.2)\n",
    "        model.fit(self.x_train,self.y_train)\n",
    "        scores3 = cross_val_score(model, self.x_train, self.y_train, cv=5, scoring='accuracy')\n",
    "        print(\"Score of Bagging SVC in Cross Validation\", scores3.mean() * 100)\n",
    "        print(\" Bagging of SVC : accurancy_is\", metrics.accuracy_score(self.y_test, model.predict(self.x_test)))\n",
    "\n",
    "def main():\n",
    "    data_raw = Reader().read(\"reddit_train.csv\")\n",
    "    data_train = data_raw['comments']\n",
    "    data_test = data_raw['subreddits']\n",
    "    print( data_test)\n",
    "    #use_lemmer,use_stemmer, use_stopwords\n",
    "    cleaner_train = Cleaner(data_train,True,False,False)\n",
    "    cleaner_train.cleaned()\n",
    "    \n",
    "    #data_x , data_y = mk(n_sampls=1000, n_features = 1000, n_informative = 10)\n",
    "    \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = Feature_Processer().split(data_train,data_test,0.9)\n",
    "    #X_train = X_train[ : 1000]\n",
    "    #X_test = X_test[1000:1200]\n",
    "    #y_train = y_train[:1000]\n",
    "    #y_test = y_test[1000:1200]\n",
    "    \n",
    "    #X_train, X_test = Feature_Processer().count_vector_features_produce(X_train,X_test,1)\n",
    "    X_train, X_test = Feature_Processer().tf_idf(X_train, X_test,(1,1),2)\n",
    "    \n",
    "    #categoryDict = dict([(y,x+1) for x,y in enumerate(sorted(set(y_train)))])\n",
    "    categoryDict = {'AskReddit': 1, 'GlobalOffensive': 2, 'Music': 3, 'Overwatch': 4, 'anime': 5, 'baseball': 6, 'canada': 7, 'conspiracy': 8, 'europe': 9, 'funny': 10, 'gameofthrones': 11, 'hockey': 12, 'leagueoflegends': 13, 'movies': 14, 'nba': 15, 'nfl': 16, 'soccer': 17, 'trees': 18, 'worldnews': 19, 'wow': 20}\n",
    "    #{'AskReddit': 1, 'GlobalOffensive': 2, 'Music': 3, 'Overwatch': 4, 'anime': 5, 'baseball': 6, 'canada': 7, 'conspiracy': 8, 'europe': 9, 'funny': 10, 'gameofthrones': 11, 'hockey': 12, 'leagueoflegends': 13, 'movies': 14, 'nba': 15, 'nfl': 16, 'soccer': 17, 'trees': 18, 'worldnews': 19, 'wow': 20}\n",
    "    #y_train = [categoryDict[i] for i in y_train]\n",
    "    #y_test = [categoryDict[i] for i in y_test]\n",
    "    \n",
    "    clf = classifier(X_train, X_test, y_train, y_test)\n",
    "    #clf.Ber_NaiveBayes(1)\n",
    "    #logistic converges deadly\n",
    "    #clf.logistic(10)\n",
    "    clf.svm(1)\n",
    "    #跑不动\n",
    "    #clf.decision_tree()\n",
    "    #Decision tree 23% 0.01 28%\n",
    "    #Decision tree     0.001 27%\n",
    "    #Decision tree     0.1 26%\n",
    "\n",
    "    #clf.multNB()\n",
    "    #svm approximately 56-57%\n",
    "    #multinomial Nb with 55-56% for removing the frequency less than 2 20% 0.0001 54%\n",
    "    #bigram with unigram 50% distrucbution 2 51%-52%  0.02 20%\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
